{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8ba66-7bd6-495f-84c3-8957721cdd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad0312-2885-4a4a-8dc7-2c5c78ecfa36",
   "metadata": {},
   "source": [
    "In the context of artificial neural networks, an activation function is a mathematical operation applied to each node (or neuron) in a neural network. The purpose of an activation function is to introduce non-linearities into the network, allowing it to learn from and model complex patterns in the data.\n",
    "\n",
    "Neural networks consist of layers of interconnected nodes, and each connection between nodes has a weight associated with it. The activation function takes the weighted sum of inputs to a node and produces an output that is passed on to the next layer of the network. The introduction of non-linear activation functions is crucial because it enables the neural network to learn and represent non-linear relationships in the data. Without non-linear activation functions, a neural network would behave like a linear model, regardless of its depth.\n",
    "\n",
    "Common activation functions include:\n",
    "\n",
    "Sigmoid (Logistic): Scales the output between 0 and 1. It's often used in the output layer for binary classification problems.\n",
    "\n",
    "Hyperbolic Tangent (tanh): Similar to the sigmoid but scales the output between -1 and 1, making it zero-centered. It is often used in hidden layers.\n",
    "\n",
    "Rectified Linear Unit (ReLU): Outputs the input for positive values and zero for negative values. It has become widely popular in hidden layers due to its simplicity and effectiveness.\n",
    "\n",
    "Leaky ReLU: Similar to ReLU, but allows a small, non-zero gradient for negative inputs to prevent dead neurons.\n",
    "\n",
    "Softmax: Used in the output layer for multi-class classification problems. It converts the network's raw output into probability distributions over multiple classes.\n",
    "\n",
    "The choice of activation function depends on the nature of the problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eadf3f-acd9-49b7-97f7-b4334c5ec061",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e202e-2635-4914-9b36-bc55ee5eb85d",
   "metadata": {},
   "source": [
    "\n",
    "Certainly! Here are some common types of activation functions used in neural networks:\n",
    "\n",
    "Sigmoid (Logistic): The sigmoid function squashes the input values to be between 0 and 1. It's often used in the output layer for binary classification problems.\n",
    "\n",
    "sigmoid\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "sigmoid(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "Hyperbolic Tangent (tanh): Similar to the sigmoid function, the tanh function squashes input values between -1 and 1. It is often used in hidden layers.\n",
    "\n",
    "tanh\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "−x\n",
    " \n",
    "e \n",
    "x\n",
    " −e \n",
    "−x\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Rectified Linear Unit (ReLU): ReLU is one of the most popular activation functions. It outputs the input for positive values and zero for negative values.\n",
    "\n",
    "ReLU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "ReLU(x)=max(0,x)\n",
    "\n",
    "Leaky ReLU: Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient for negative inputs to prevent dead neurons.\n",
    "\n",
    "Leaky ReLU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "Leaky ReLU(x)=max(αx,x)\n",
    "\n",
    "where \n",
    "�\n",
    "α is a small positive constant.\n",
    "\n",
    "Parametric ReLU (PReLU): Similar to Leaky ReLU, but the slope for negative values is learned during training.\n",
    "\n",
    "PReLU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "PReLU(x)=max(αx,x)\n",
    "\n",
    "where \n",
    "�\n",
    "α is a learnable parameter.\n",
    "\n",
    "Exponential Linear Unit (ELU): ELU is another variant of ReLU that smoothens the transition for negative values and allows small negative values.\n",
    "\n",
    "ELU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "{\n",
    "�\n",
    "if \n",
    "�\n",
    "≥\n",
    "0\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    ")\n",
    "if \n",
    "�\n",
    "<\n",
    "0\n",
    "ELU(x)={ \n",
    "x\n",
    "α(e \n",
    "x\n",
    " −1)\n",
    "​\n",
    "  \n",
    "if x≥0\n",
    "if x<0\n",
    "​\n",
    " \n",
    "\n",
    "where \n",
    "�\n",
    "α is a hyperparameter.\n",
    "\n",
    "Softmax: The softmax function is commonly used in the output layer for multi-class classification problems. It converts the raw output of the network into a probability distribution over multiple classes.\n",
    "\n",
    "Softmax\n",
    "(\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "∑\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "Softmax(x \n",
    "i\n",
    "​\n",
    " )= \n",
    "∑ \n",
    "j\n",
    "​\n",
    " e \n",
    "x \n",
    "j\n",
    "​\n",
    " \n",
    " \n",
    "e \n",
    "x \n",
    "i\n",
    "​\n",
    " \n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "The choice of activation function depends on the specific characteristics of the problem, and experimentation is often necessary to determine the most effective function for a given task.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8542839-006a-4333-a53e-0409c8b89c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ba375-199c-46aa-ab66-15c6475213dd",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in the training process and performance of a neural network. They introduce non-linearities to the network, allowing it to learn complex relationships and patterns in the data. Here are some key ways in which activation functions impact neural network training and performance:\n",
    "\n",
    "Non-linearity: Activation functions introduce non-linearities to the network, enabling it to model and approximate complex, non-linear relationships in the data. Without non-linear activation functions, the entire neural network would behave like a linear model, limiting its expressive power.\n",
    "\n",
    "Gradient Descent Optimization: During the training process, neural networks use optimization algorithms like gradient descent to minimize the error or loss function. Activation functions affect the gradients that are propagated backward through the network during the backpropagation algorithm. The choice of activation function can influence how well the gradients flow through the network, impacting the convergence of the optimization algorithm.\n",
    "\n",
    "Vanishing and Exploding Gradients: Some activation functions can suffer from the vanishing gradient problem or exploding gradient problem. The vanishing gradient problem occurs when gradients become too small during backpropagation, leading to slow or stalled learning in the early layers of the network. The exploding gradient problem happens when gradients become too large, causing instability during training. Activation functions like sigmoid and hyperbolic tangent (tanh) are more prone to vanishing gradients, while others like Rectified Linear Unit (ReLU) may suffer from exploding gradients in certain situations.\n",
    "\n",
    "Sparse Activation: Activation functions can also influence the sparsity of the activations in a neural network. Sparsity refers to the proportion of zero-valued elements in a matrix or vector. Activation functions like ReLU tend to produce sparse activations, which can be beneficial for reducing computational complexity and improving efficiency.\n",
    "\n",
    "Output Range: The range of values an activation function can produce affects the behavior of the network. For example, sigmoid functions squash their input values to the range (0, 1), which is suitable for binary classification problems. On the other hand, tanh functions map input values to the range (-1, 1), making them useful for problems where the output should be centered around zero.\n",
    "\n",
    "Robustness to Input Variations: Different activation functions respond differently to variations in input. Some activation functions are more robust to small changes in input, which can be beneficial for generalization and handling noisy data.\n",
    "\n",
    "Popular activation functions include sigmoid, hyperbolic tangent (tanh), Rectified Linear Unit (ReLU), and variants like Leaky ReLU and Parametric ReLU (PReLU), among others. The choice of activation function depends on the specific characteristics of the data and the requirements of the task at hand. Experimentation with different activation functions is often necessary to find the most suitable one for a particular neural network architecture and problem domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696f5b8-a511-4b5c-abe2-ed90e047fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23f54d-6ce8-4d53-a740-84d827b665cc",
   "metadata": {},
   "source": [
    "\n",
    "The sigmoid activation function, also known as the logistic function, is a widely used non-linear activation function in neural networks. It takes an input value and squashes it to a range between 0 and 1. The sigmoid function is defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "σ(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "where \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "σ(x) is the output of the sigmoid function, and \n",
    "�\n",
    "e is the base of the natural logarithm.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Range: The output of the sigmoid function always lies in the range (0, 1). This makes it suitable for binary classification problems, where the goal is to produce an output that can be interpreted as a probability.\n",
    "\n",
    "Smoothness: The sigmoid function is smooth and differentiable everywhere, which is important for gradient-based optimization algorithms like backpropagation.\n",
    "\n",
    "Activation Threshold: The sigmoid function can be interpreted as an activation threshold. If the output is close to 0, the neuron's activation is close to off, and if the output is close to 1, the neuron's activation is close to on.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Output Range: The sigmoid function outputs values between 0 and 1, making it convenient for binary classification problems where the goal is to predict probabilities.\n",
    "\n",
    "Smoothness: The smoothness of the sigmoid function allows for gradient-based optimization methods like gradient descent to be applied during the training process.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Vanishing Gradient: One significant disadvantage of the sigmoid function is the vanishing gradient problem. During backpropagation, as the sigmoid function saturates for extreme input values, the gradient becomes very small. This can result in slow or stalled learning, especially in deep neural networks.\n",
    "\n",
    "Not Zero-Centered: The sigmoid function is not zero-centered, meaning that the output is not centered around zero. This can lead to issues in weight updates during optimization and may make convergence slower.\n",
    "\n",
    "Output Saturation: The sigmoid function saturates for extreme input values, causing the output to be very close to 0 or 1. This can result in difficulties during learning as the network may have a hard time updating weights for saturated neurons.\n",
    "\n",
    "Output Bias: The output of the sigmoid function is in the range (0, 1), which may lead to issues like the \"vanishing gradient\" problem when used in certain architectures or deep networks.\n",
    "\n",
    "Due to its vanishing gradient and other limitations, the sigmoid activation function is often replaced by alternatives like the hyperbolic tangent (tanh) or Rectified Linear Unit (ReLU) in hidden layers of neural networks. However, it is still commonly used in the output layer for binary classification problems.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77c5f8-3138-470e-af9f-8a9b3b3cc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de6006-1dae-41a6-8169-15ee0415d99a",
   "metadata": {},
   "source": [
    "The Rectified Linear Unit (ReLU) is an activation function commonly used in neural networks, especially in the hidden layers. It addresses some of the limitations of activation functions like the sigmoid. The ReLU function is defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "f(x)=max(0,x)\n",
    "\n",
    "In other words, the output of the ReLU function is the maximum of 0 and the input \n",
    "�\n",
    "x. Graphically, it looks like a piecewise linear function, producing a linear response for positive inputs and outputting zero for negative inputs.\n",
    "\n",
    "Key characteristics and differences from the sigmoid function:\n",
    "\n",
    "Non-linearity: Like the sigmoid function, ReLU introduces non-linearity to the network. However, ReLU is not bound between 0 and 1 and has a more straightforward form.\n",
    "\n",
    "Vanishing Gradient: ReLU addresses the vanishing gradient problem better than the sigmoid function. The derivative of ReLU is either 0 for negative inputs or 1 for positive inputs. This leads to more effective gradient flow during backpropagation, allowing for faster and more efficient training of deep neural networks.\n",
    "\n",
    "Sparsity: ReLU tends to produce sparse activations. If the input is negative, the output is zero, effectively sparsifying the activations. This sparsity can be beneficial for reducing computational complexity and improving efficiency.\n",
    "\n",
    "Computational Efficiency: ReLU is computationally efficient because it involves simple operations (max function and comparison) compared to the more complex operations involved in the sigmoid or tanh functions.\n",
    "\n",
    "Not Zero-Centered: One potential drawback of ReLU is that it is not zero-centered, meaning that the output is always positive or zero. This lack of zero-centering can lead to issues in weight updates during optimization, and some variants of ReLU, such as Leaky ReLU, aim to address this by allowing a small negative slope for negative inputs.\n",
    "\n",
    "Output Range: The output of ReLU is unbounded on the positive side, making it more suitable for situations where the model needs to learn complex and diverse patterns without being constrained to a specific range.\n",
    "\n",
    "In summary, ReLU is a popular choice for activation functions in hidden layers due to its ability to mitigate the vanishing gradient problem and its computational efficiency. However, it may not be suitable for all scenarios, and variants like Leaky ReLU or Parametric ReLU are sometimes used to address specific issues associated with the standard ReLU.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef7ca2-8a5e-45bb-95b3-33b600525c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cef6b-7c56-40c1-91cc-65e988858de2",
   "metadata": {},
   "source": [
    "Using the Rectified Linear Unit (ReLU) activation function over the sigmoid function offers several benefits, especially in the context of training deep neural networks. Here are some key advantages of ReLU over sigmoid:\n",
    "\n",
    "Mitigation of Vanishing Gradient Problem:\n",
    "\n",
    "The sigmoid function has a tendency to saturate and squash gradients for extreme input values, leading to the vanishing gradient problem. This can cause slow or stalled learning in deep networks.\n",
    "ReLU, on the other hand, has a constant gradient of 1 for positive inputs, which helps mitigate the vanishing gradient problem. This allows for more effective backpropagation and facilitates the training of deeper architectures.\n",
    "Computational Efficiency:\n",
    "\n",
    "ReLU is computationally more efficient compared to the sigmoid function.\n",
    "The ReLU activation involves simple operations like the max function and comparison, making it faster to compute during both forward and backward passes.\n",
    "Sparsity:\n",
    "\n",
    "ReLU tends to produce sparse activations. If the input is negative, the output is zero, leading to sparsity in the network.\n",
    "Sparse activations can be beneficial for reducing computational complexity and memory requirements, as only a subset of neurons are activated for a given input.\n",
    "Biological Plausibility:\n",
    "\n",
    "ReLU has been argued to be more biologically plausible as an activation function when compared to the sigmoid function.\n",
    "Biological neurons exhibit a type of rectification behavior, where they are either activated or not, without a gradual transition. ReLU captures this behavior more closely.\n",
    "Avoidance of Saturation:\n",
    "\n",
    "Sigmoid saturates for extreme input values, leading to very small gradients during backpropagation. This saturation can slow down learning.\n",
    "ReLU does not saturate for positive inputs, allowing the model to learn faster and preventing issues associated with saturation.\n",
    "Output Range:\n",
    "\n",
    "The output of ReLU is unbounded on the positive side, which can be beneficial for models that need to learn complex and diverse patterns without being constrained to a specific range (as opposed to the sigmoid, which outputs values in the range [0, 1]).\n",
    "Zero-Centered Alternatives:\n",
    "\n",
    "While standard ReLU is not zero-centered (meaning its outputs are always non-negative), there are variants like Leaky ReLU or Parametric ReLU that introduce a small negative slope for negative inputs, addressing the issue of lack of zero-centeredness.\n",
    "It's important to note that the choice of activation function depends on the specific characteristics of the data and the requirements of the task at hand. While ReLU offers advantages in many scenarios, there are also cases where other activation functions, or even variations of ReLU, may be more suitable. It is common to experiment with different activation functions to determine the one that works best for a particular neural network architecture and problem domain.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63bad3-c766-43d7-add4-07daea1f6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1df32-01e4-44ab-b06c-3abfc2e8d471",
   "metadata": {},
   "source": [
    "Rectified Linear Unit (ReLU) is a popular activation function used in artificial neural networks. The basic form of ReLU is defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "f(x)=max(0,x)\n",
    "\n",
    "However, ReLU has a drawback known as the \"dying ReLU\" problem. In this issue, neurons can sometimes become inactive, always outputting zero. This can happen during training when the input to a ReLU unit is consistently negative. In such cases, the gradient flowing through the unit is always zero, and the weights are not updated during the training process. This leads to a situation where the neuron becomes \"dead\" and stops learning.\n",
    "\n",
    "Leaky ReLU is a modification of the traditional ReLU activation function that aims to address the dying ReLU problem. It introduces a small slope for the negative part of the function, allowing a small, non-zero gradient when the input is negative. The leaky ReLU function is defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "{\n",
    "�\n",
    ",\n",
    "if \n",
    "�\n",
    ">\n",
    "0\n",
    "�\n",
    "�\n",
    ",\n",
    "otherwise\n",
    "f(x)={ \n",
    "x,\n",
    "αx,\n",
    "​\n",
    "  \n",
    "if x>0\n",
    "otherwise\n",
    "​\n",
    " \n",
    "\n",
    "where \n",
    "�\n",
    "α is a small positive constant, usually a small fraction like 0.01.\n",
    "\n",
    "The introduction of a small slope for negative values in leaky ReLU ensures that even when the input is negative, there is still a gradient flowing through the unit. This helps to overcome the vanishing gradient problem, which is a common issue in deep neural networks during backpropagation. The vanishing gradient problem occurs when the gradients become extremely small as they are propagated back through the network during training, making it difficult for the lower layers to learn effectively.\n",
    "\n",
    "By allowing a small, non-zero gradient for negative inputs, leaky ReLU helps to mitigate the vanishing gradient problem and allows the network to continue learning even for negative inputs. This property makes leaky ReLU a popular choice in certain neural network architectures, particularly when the vanishing gradient problem is a concern.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f409559-a1f6-4521-b6f5-f43fe33184c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d42556-2296-4f03-b56e-a1dcb9e1fa75",
   "metadata": {},
   "source": [
    "The softmax activation function is commonly used in the output layer of a neural network for multi-class classification problems. Its purpose is to convert a vector of raw scores or logits into a probability distribution over multiple classes. The softmax function takes as input a vector of real numbers and outputs a probability distribution that sums to 1.\n",
    "\n",
    "The formula for the softmax function for a vector \n",
    "�\n",
    "=\n",
    "(\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "z=(z \n",
    "1\n",
    "​\n",
    " ,z \n",
    "2\n",
    "​\n",
    " ,…,z \n",
    "k\n",
    "​\n",
    " ) is given by:\n",
    "\n",
    "softmax\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "softmax(z) \n",
    "i\n",
    "​\n",
    " = \n",
    "∑ \n",
    "j=1\n",
    "k\n",
    "​\n",
    " e \n",
    "z \n",
    "j\n",
    "​\n",
    " \n",
    " \n",
    "e \n",
    "z \n",
    "i\n",
    "​\n",
    " \n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "where \n",
    "softmax\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "softmax(z) \n",
    "i\n",
    "​\n",
    "  is the i-th element of the resulting probability distribution.\n",
    "\n",
    "Key points about the softmax function:\n",
    "\n",
    "Normalization: The exponential function is used to ensure that the output probabilities are positive. The division by the sum of exponentials normalizes the values, ensuring that the resulting probabilities sum to 1. This normalization property is crucial for interpreting the output as probabilities.\n",
    "\n",
    "Relative Scale: The softmax function retains the relative ordering of the input values, meaning that larger input values will correspond to larger probabilities in the output distribution. This is important for identifying the most likely class in a classification task.\n",
    "\n",
    "Common use cases for the softmax activation function include:\n",
    "\n",
    "Multi-Class Classification: Softmax is widely used in the output layer of neural networks when the task involves classifying input data into multiple classes. The predicted probabilities for each class are obtained using softmax, and the class with the highest probability is chosen as the final prediction.\n",
    "\n",
    "Neural Language Models: In natural language processing tasks, such as language modeling or text generation, softmax is often used in the output layer to predict the next word or character in a sequence of text.\n",
    "\n",
    "Image Classification: In computer vision applications, softmax is commonly used to predict the probabilities of different classes in image classification tasks.\n",
    "\n",
    "In summary, the softmax activation function is employed to produce a probability distribution over multiple classes, making it well-suited for multi-class classification problems in various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13362ab9-ac1f-4cd1-a890-d407f5325db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251570e-a6e1-420e-8582-22b932fd1d52",
   "metadata": {},
   "source": [
    "The hyperbolic tangent, often abbreviated as tanh, is an activation function used in artificial neural networks. It is a variant of the sigmoid function and is defined as:\n",
    "\n",
    "tanh\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "2\n",
    "�\n",
    "−\n",
    "1\n",
    "�\n",
    "2\n",
    "�\n",
    "+\n",
    "1\n",
    "tanh(x)= \n",
    "e \n",
    "2x\n",
    " +1\n",
    "e \n",
    "2x\n",
    " −1\n",
    "​\n",
    " \n",
    "\n",
    "Here are some key characteristics and a comparison with the sigmoid function:\n",
    "\n",
    "Range:\n",
    "\n",
    "The tanh function has a range between -1 and 1, mapping input values to the interval \n",
    "(\n",
    "−\n",
    "1\n",
    ",\n",
    "1\n",
    ")\n",
    "(−1,1).\n",
    "The sigmoid function, on the other hand, has a range between 0 and 1, mapping input values to the interval \n",
    "(\n",
    "0\n",
    ",\n",
    "1\n",
    ")\n",
    "(0,1).\n",
    "Zero-Centered:\n",
    "\n",
    "One notable difference between tanh and the sigmoid function is that tanh is zero-centered. This means that the average output of the tanh activation is centered around zero, which can be beneficial for the optimization process during training.\n",
    "In contrast, the sigmoid function is not zero-centered, as its outputs are biased towards one end (close to 0 or 1).\n",
    "Symmetry:\n",
    "\n",
    "The tanh function is symmetric around the origin (0,0). That is, \n",
    "tanh\n",
    "(\n",
    "−\n",
    "�\n",
    ")\n",
    "=\n",
    "−\n",
    "tanh\n",
    "(\n",
    "�\n",
    ")\n",
    "tanh(−x)=−tanh(x).\n",
    "The sigmoid function is not symmetric.\n",
    "Gradient Magnitude:\n",
    "\n",
    "The gradient of the tanh function is generally stronger than the gradient of the sigmoid function. This means that the tanh function can exhibit stronger gradients during backpropagation, potentially aiding in the learning process.\n",
    "The sigmoid function has smaller gradients, especially in the saturating regions (near 0 or 1), which can lead to slower convergence during training (vanishing gradient problem).\n",
    "In summary, while both the tanh and sigmoid functions are commonly used activation functions in neural networks, the tanh function provides outputs in the range of \n",
    "(\n",
    "−\n",
    "1\n",
    ",\n",
    "1\n",
    ")\n",
    "(−1,1) and is zero-centered, offering certain advantages during training. The choice between tanh and sigmoid depends on the specific requirements of the neural network architecture and the characteristics of the data being processed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254035e-32c9-4fe9-bd3c-3762b89b16e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de88118-cb84-4754-922c-34faf1aaa8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b2df4-5b7b-4cfc-90dc-a5dfd0d40fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff6af6-7bc6-418a-93ec-9c8aff8a8262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd90f35-a9d1-46ab-a4fa-ae94732ce187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8887e-82d8-4be7-8af0-82d786dc10a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6f7ac-1761-4982-b43b-3a015da0442d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc661b-12e2-4471-a9a6-a93b822ff748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce777b-522b-4054-840b-ad309ef9adaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
